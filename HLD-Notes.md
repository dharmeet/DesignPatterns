# HLD Notes

## Topic wise

| Topic | Description | Use-case |
| ----- | ---------- | --------- |
| Consistent Hashing | multiple hash functions for servers and single hash function for user_id | Used in case of sharding is done by user_id |
|  Load Balancer | Listener (entry point) - Routing (decision making process based on rules and algorithms) - Target Group (collection of servers that actually process the request), Reverse Proxy, Security, Performance, Resilience  | |
| In-Browser Caching | DNS and static information like image, videos and JS files | Mostly used in all the websites |
| CDN (Content Delivery Network) | Store the data, distribute it to all the regions and provide different CDN links to access data in a particular region | Video Streaming like Hoststar, Youtube etc. |
| Local Caching | Caching done on the app server | Tests input and output files for a problem on Scaler, Hackerrank etc. Metadata DB stores the last updated time and compares if we need to get the file from file storage or local cache. | 
| Global Caching | System like Redis, Memcache. Used mostly for caching something that is queried often, storing derived information which might be expensive to compute on DB. | Contest ranklist - compute the ranklist periodically (eg. 1 min) and update it in Redis, this way there would be only one cache miss every minute. |
| Cache with TTL | Entries in the cache will be valid for only a period, after that if you need it you fetch it again. | Weather Data, Caching currency exchange rates |
| Write through cache | Writing in cache and DB, returning success if both are successful. Writes are slower but reads are much faster. | Read-heavy system, E-commerce inventory update |
| Write back cache | First write in cache, then asynchronously sync with DB. Very high throughput and very low latency. | Social media feeds/counters (eg. likes, count), Video streaming watch history |
| Write around cache | Writes are done directly in the DB, can use TTL to keep cache in sync with the DB | Logging system, Infrequently accessed archival data |
| CAP Theorem | Distributed system can only provide two of three properties simultaneously Consistency, Availability and Partition tolerance. Consistency - return success only if both have written, Availability - mark the inactive node as active only when it has synced the data with others, Network Partitioning - one has to choose between availability (A) and consistency (C), if there is no network partitioning we have to choose between extremely low Latency or high Consistency. | (a) In banking system, consistency is important, we want immediate consistency but in reality ATM transactions use eventual consistency. (b) In FB newsfeed like system, availability is more important than consistency. (c) For Quora, availability is more important. (d) For Facebook messenger, consistency is very important. |
| Master slave | All writes first come to master, reads can go to any machine, whenever master dies a new election of the master will take place based on election algorithms. | |
| Master slave highly available not eventually consistent | Master takes the write, if successful return success. Try to sync with slaves. | Log system, like Splunk, throughput is high we need to process the logs even if we lose some logs, it's ok. |
| Master slave highly available and eventually consistent | Master takes the write and if one slave writes success is returned. | FB newsfeed and storing posts, we don't want post be lost, they could be delayed but eventually sync up. |
| Master slave highly consistent | Master and all slaves take the writes, if all have written then only return success. | Banking system. |
| SQL | Normalization and ACID transactions are the advantages of SQL. On the other hand, fixed schema might not fit every use case, also if we need to do DB sharding then it nullifies the advantages of sharding. | |
| Points to note while choosing sharding key | 1. Load should be evenly distributed. 2. Most frequent operations should be performed efficiently. 3. Minimum machines should be updated when most frequent operations are performed. 4. Minimize redundancy as much as possible.  | Banking system, user_id is better sharding key then city_id. Uber-like system, use case is to search for nearby drivers, CityId would be a good sharding key compared to driverId. Slack (Groups-heavy system) GroupId is the good sharding key. IRCTC Sharding key - primary problem is to avoid double-booked tickets, load balancing during tatkal booking. TrainId is a good sharding key, load gets distributed among all machines, tomorrow there would be lot of trains, within a train it knows which user is allocated a particular berth. |
| Types of NoSQL DB | 1. Key-value NoSQL DBs (eg. DynamoDB, Redis etc.)  2. Document DBs structures data in JSON format, every record is JSON and all records can have different attributes (eg. MongoDB, AWS ElasticSearch, Azure Cosmos).  3. Column-Family Storage (eg. Cassandra HBase) - RowID is the sharding key, within RowID there are bunch of column families which contains records, and record have timestamp at the beginning. | Document DB is mostly used in ecommerce applications. Column Family allows prefix search and fetching top or latest X entries efficiently. Twitter-Hashtag data storage - Column-family is a better choice, as with Key Value it will fetch all the tweets for given hashtag, whereas we want only 20 pagination. Live scores of Sports/Matches - Key Value pair is the best choice. Current location of Cab in Uber-like Application - if location history is needed column family is the best choice, if only current location is needed key value DB is the best choice. |
| Manual Sharding | Order of priority for extra machines: maintain the replication level > create a new shard > keep them in standby. Shard Creation involves Staging Phase when the shard is not live, and then involves the Real Phase in which shard is live and catches up the data between T1 and T2. | NameNode (Hadoop), JobTracker, HBase Master |
| Multi Master | Every machine in the system is a master. Tunable consistency, R (minimum number of read operations required before calling it successful) W (minimum number of write operations required before calling it successful) X (replication level). R <= X and W <= X. Lower R + W, lower consistency, higher R + W higher consistency. If R + W > X then it is highly consistent system. | DynamoDB, Cassandra |
| LSM Tree (Log-Structured Merge) Tree supports high write throughput | In-memory component - memtable (balanced tree), On-disk Component - multiple immutable sorted files called SSTables (Sorted String Tables), Multiple levels.  **Writes** are first buffered to an in-memory memtable, when memtable reaches capacity, it's flushed to disk as an immutable SSTable, background process periodically merges smaller SSTables into larger ones (compaction). Writes are immediately acknowledged after being written to memtable and WAL (Write-Ahead Log).  **For read**, check memtable first, if not found check SSTable in level order (newest to oldest), use Bloom filters to avoid unecessary disk accesses. May require checking multiple SSTables. | Advantages: Very high write throughput, works well with SSD and HDD storage, tunable performance between reads and writes via compaction strategies. LSM Trees are used in Cassandra, RocksDB, LevelDB and HBase. |
| Bloom Filters | **Space-efficient** Requires significantly less memory than storing the actual elements. **Probabilistic** Can have false positives but never false negatives. **Constant time operations** O(k) where k is the number of hash functions used. How it works: 1. Start with a bit array of m bits, all set to 0. 2. Use k different hash functions to map each element to k positions in the array. 3. To add an element, set all k corresponding bits to 1. 4. To check if an element exists: a. If any of the k bits is 0, the element is definitely not in the set. b. If all k bits are 1, element might be present in SSTable.| Used in LSM Tree, for searching an element in SSTable. |

## How to approach a system design problem:
1. Figure out the MVP.  
2. Estimate scale 
    a. Storage requirements (Is sharding needed?) 
    b. Read-heavy or write-heavy system. (Write operations block read requests, as they acquire a lock on impacted rows, if system is both read and write heavy then figure out how to absorb reads or writes somewhere else.) 
    c. QPS  
3. Design Goal 
    a. Highly consistent or Highly Available. 
    b. Latency requirements 
    c. Can we afford data loss?  
4. API (the choice of sharding key may depend on the API parameters.)

## Case Studies

| S. No. | Problem | Approach |
| ----- | ---------- | --------- |
| 1 | Submit problem on Hackerrank, scaler (etc.) the input and output file is stored in File Storage which can take 2 seconds to fetch | Reading the file from Hard Disk takes 40 ms and reading a record from DB takes 50 ms. Store the file metadata in DB, with problem_id, input_filepath, input_file_updated_at, input_file_created_at and keep the filename as (problem_id)_(updated_at)_input.txt, now when the submission comes check the updated_at in the DB, if the file is present in local cache we are good, else fetch it from the filesystem. |
| 2 | How facebook computes its newsfeed (posts made by friends of the user) ? |  Posts made by user would be very less as compared to number of active users (80-20-1 rule), 1% will do posts, 80% reading, 20% interact. MAU - 1 Billion, DAU - 500 million, posts writing 1% - 5 million, lets say each person writes 4 posts daily, so we have 20 million posts every day. Each post is ~ 300 bytes, so space required for posts generated in a day 20 Mn * 300 = 6 GB. Assume only need to show posts made in last 30 days, so total space needed is 180 GB for 30 days. Hence, posts can be stored in a separate DB and retrieving becomes easier from the derived data. So recent posts, can be stored on multiple machines and separate DB for user data, so to give the recent posts we first get the friend_ids of the user, then select recent posts made by the user's friend SELECT * FROM all_posts WHERE user_id IN friend_ids LIMIT x OFFSET y. Here we are caching the posts in the HDD, not in RAM, but still this is much faster than getting the data from an actual storage system. |  
| 3 | Search Typeahead System - Max 5 suggestions required, most popular ones, strict prefix match, assume suggestions will be shown post 3 characters. Scale - 10 Billion search queries in a day, which translates to 60 billion typeahead queries in a day, if one search triggers 6 typeahead queries. Design Goal - Availability is more important than consistency, latency of getting suggestions should be super low.  Need of sharding? Assume 10% of the queries contain new search terms ~ 1 billion new search terms every day ~ 365 billion in a year, designing for 10 years 365 * 10 billion, each search term is 32 bytes and frequency is 8 bytes ~ 40 * 10 * 365 billion bytes ~ 146 TB (Sharding is needed.) | API: getSuggestion(prefix_term, limit =5), updateFrequency(search_term) If we store using Trie, so then "mic", will have a huge subtree, but we can store top five suggestions in each node, with this getSuggestion API would be quite fast, but in case of Trie we would have to take care of sharding ourselves. If we store using a HashMap, then we would need two hashmaps one for frequency, and other with top5suggestions. But with the hashmap, sharding is straight forward, but we can try to buffer the writes of updateFrequency to a separate map with the threshold, and then update the frequency in SearchTermFrequency map and PrefixTopSuggestions map if the threshold of 50 is crossed. This will take care of recency as well and optimize the number of writes. Along with this, we would also need sampling, as we are only interested in trending words rather than their exact frequency. To take recency of search queries into consideration, we can keep a Time Decay Factor to reduce the weightage of search queries performed earlier. |