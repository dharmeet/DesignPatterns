# HLD Notes

## Topic wise

| Topic | Description | Use-case |
| ----- | ---------- | --------- |
| Consistent Hashing | multiple hash functions for servers and single hash function for user_id | Used in case of sharding is done by user_id |
|  Load Balancer | Listener (entry point) - Routing (decision making process based on rules and algorithms) - Target Group (collection of servers that actually process the request), Reverse Proxy, Security, Performance, Resilience  | |
| In-Browser Caching | DNS and static information like image, videos and JS files | Mostly used in all the websites |
| CDN (Content Delivery Network) | Store the data, distribute it to all the regions and provide different CDN links to access data in a particular region | Video Streaming like Hoststar, Youtube etc. |
| Local Caching | Caching done on the app server | Tests input and output files for a problem on Scaler, Hackerrank etc. Metadata DB stores the last updated time and compares if we need to get the file from file storage or local cache. | 
| Global Caching | System like Redis, Memcache. Used mostly for caching something that is queried often, storing derived information which might be expensive to compute on DB. | Contest ranklist - compute the ranklist periodically (eg. 1 min) and update it in Redis, this way there would be only one cache miss every minute. |
| Cache with TTL | Entries in the cache will be valid for only a period, after that if you need it you fetch it again. | Weather Data, Caching currency exchange rates |
| Write through cache | Writing in cache and DB, returning success if both are successful. Writes are slower but reads are much faster. | Read-heavy system, E-commerce inventory update |
| Write back cache | First write in cache, then asynchronously sync with DB. Very high throughput and very low latency. | Social media feeds/counters (eg. likes, count), Video streaming watch history |
| Write around cache | Writes are done directly in the DB, can use TTL to keep cache in sync with the DB | Logging system, Infrequently accessed archival data |
| CAP Theorem | Distributed system can only provide two of three properties simultaneously Consistency, Availability and Partition tolerance. Consistency - return success only if both have written, Availability - mark the inactive node as active only when it has synced the data with others, Network Partitioning - one has to choose between availability (A) and consistency (C), if there is no network partitioning we have to choose between extremely low Latency or high Consistency. | (a) In banking system, consistency is important, we want immediate consistency but in reality ATM transactions use eventual consistency. (b) In FB newsfeed like system, availability is more important than consistency. (c) For Quora, availability is more important. (d) For Facebook messenger, consistency is very important. |
| Master slave | All writes first come to master, reads can go to any machine, whenever master dies a new election of the master will take place based on election algorithms. | |
| Master slave highly available not eventually consistent | Master takes the write, if successful return success. Try to sync with slaves. | Log system, like Splunk, throughput is high we need to process the logs even if we lose some logs, it's ok. |
| Master slave highly available and eventually consistent | Master takes the write and if one slave writes success is returned. | FB newsfeed and storing posts, we don't want post be lost, they could be delayed but eventually sync up. |
| Master slave highly consistent | Master and all slaves take the writes, if all have written then only return success. | Banking system. |
| SQL | Normalization and ACID transactions are the advantages of SQL. On the other hand, fixed schema might not fit every use case, also if we need to do DB sharding then it nullifies the advantages of sharding. | |
| Points to note while choosing sharding key | 1. Load should be evenly distributed. 2. Most frequent operations should be performed efficiently. 3. Minimum machines should be updated when most frequent operations are performed. 4. Minimize redundancy as much as possible.  | Banking system, user_id is better sharding key then city_id. Uber-like system, use case is to search for nearby drivers, CityId would be a good sharding key compared to driverId. Slack (Groups-heavy system) GroupId is the good sharding key. IRCTC Sharding key - primary problem is to avoid double-booked tickets, load balancing during tatkal booking. TrainId is a good sharding key, load gets distributed among all machines, tomorrow there would be lot of trains, within a train it knows which user is allocated a particular berth. |
| Types of NoSQL DB | 1. Key-value NoSQL DBs (eg. DynamoDB, Redis etc.)  2. Document DBs structures data in JSON format, every record is JSON and all records can have different attributes (eg. MongoDB, AWS ElasticSearch, Azure Cosmos).  3. Column-Family Storage (eg. Cassandra HBase) - RowID is the sharding key, within RowID there are bunch of column families which contains records, and record have timestamp at the beginning. | Document DB is mostly used in ecommerce applications. Column Family allows prefix search and fetching top or latest X entries efficiently. Twitter-Hashtag data storage - Column-family is a better choice, as with Key Value it will fetch all the tweets for given hashtag, whereas we want only 20 pagination. Live scores of Sports/Matches - Key Value pair is the best choice. Current location of Cab in Uber-like Application - if location history is needed column family is the best choice, if only current location is needed key value DB is the best choice. |
| Manual Sharding | Order of priority for extra machines: maintain the replication level > create a new shard > keep them in standby. Shard Creation involves Staging Phase when the shard is not live, and then involves the Real Phase in which shard is live and catches up the data between T1 and T2. | NameNode (Hadoop), JobTracker, HBase Master |
| Multi Master | Every machine in the system is a master. Tunable consistency, R (minimum number of read operations required before calling it successful) W (minimum number of write operations required before calling it successful) X (replication level). R <= X and W <= X. Lower R + W, lower consistency, higher R + W higher consistency. If R + W > X then it is highly consistent system. | DynamoDB, Cassandra |
| LSM Tree (Log-Structured Merge) Tree supports high write throughput | In-memory component - memtable (balanced tree), On-disk Component - multiple immutable sorted files called SSTables (Sorted String Tables), Multiple levels.  **Writes** are first buffered to an in-memory memtable, when memtable reaches capacity, it's flushed to disk as an immutable SSTable, background process periodically merges smaller SSTables into larger ones (compaction). Writes are immediately acknowledged after being written to memtable and WAL (Write-Ahead Log).  **For read**, check memtable first, if not found check SSTable in level order (newest to oldest), use Bloom filters to avoid unecessary disk accesses. May require checking multiple SSTables. | Advantages: Very high write throughput, works well with SSD and HDD storage, tunable performance between reads and writes via compaction strategies. LSM Trees are used in Cassandra, RocksDB, LevelDB and HBase. |
| Bloom Filters | **Space-efficient** Requires significantly less memory than storing the actual elements. **Probabilistic** Can have false positives but never false negatives. **Constant time operations** O(k) where k is the number of hash functions used. How it works: 1. Start with a bit array of m bits, all set to 0. 2. Use k different hash functions to map each element to k positions in the array. 3. To add an element, set all k corresponding bits to 1. 4. To check if an element exists: a. If any of the k bits is 0, the element is definitely not in the set. b. If all k bits are 1, element might be present in SSTable.| Used in LSM Tree, for searching an element in SSTable. |
| Storing large files - choose proper chunk size to avoid issues like concatenating too many files, too much cost of keeping the metadata information if chunk size is small (eg. 1 MB) | **HDFS** The default chunk size is 128 MB, the metadata information is maintained in **NameNode server**. The data is replicated on different racks so that we do not lose our data even if a rack goes down. **Upload** - The app server gets the chunk size from NameNode server, then when the data reaches chunk size or EOF, it asks about where to store the data, then metadata about the chunk is stored in the name node server first and then chunk is sent to the data node. **Download** is similar to upload get the metadata from the name node server first, then get the chunks one by one. | It has lot of applications, especially in storing large log files, or large media files in online streaming. |
| Nearest Neighbors - Quad Tree | The whole world is divided into  a quad tree, the points (places, restaurants etc.) belong to the leaf nodes, each node is divided into 4 nodes if it has more than the threshold (say 100) number of points. The height of the tree would be ~log(N), where N is the number of places in the world. **Finding gridID** Assume whole world extends between (x1, y1) and (x2, y2), i.e. root.left.corner and root.right.corner, now find Xmid = (x1 + x2)/2 and Ymid = (y1+y2)/2, for any given point (x, y) if x is bigger than Xmid, then point will be present in part2 or part4, i.e. root.children[1] or root.children[3], which can be found out by comparing y with Ymid, and recursively call it till we reach the leaf node. Now once we have the gridId, we can easily get the nearby places with the same gridId from the SQL DB. **Consider neighboring grids** If the points are not enough in the grid, we can consider neighboring grids, one way is to store next pointer for siblings of the leaf nodes, else we can consider finding the grid by changing x and y to x + 0.1, y + 0.1 **Adding a new place** - Find the gridId, add the new place if the count is less than threshold we are good, else split the current grid into four grids and update the gridId for all the 100 (threshold) places in the DB. **Delete existing place** - Similar to addition but it is not that common. | **Storage** - For every node we need to store top left and bottom right coordinates and 4 pointers to the child node. 100 million places stored exactly one. So even if each leaf node contains only 1 place, then also we would have (1/(1 - 1/4))*10^8 nodes, and if each leaf contains ~ 20 places, then we will have 6.5 million nodes. Each node requires 4*4 (child pointers) + 16*4 (coordinates) = 80 bytes, and  1 place needs 32 bytes, so total storage would be 6.5 million * 80 + 100 million * 32 = 520 * 10^8 bytes + 3200 * 10^8 bytes = ~ 4 GB. **Applications** - Google Maps finding nearby places, Cab taxi service.  |
| Zoo Keeper - What happens if master dies in the master-slave architecture, who will the app servers send write requests? | Zookeeper - Storage in Zookeeper is exactly like a file system. Inside the root folder we have bunch of files or directories, all the files are called **ZK Nodes** There are two types of ZK Nodes: 1. **Ephemeral** nodes are the files where data written is only valid till the machine/session which wrote the data is alive/active, the machine sends heartbeat to this node periodically. The ephemeral node has exactly on session/machine as the owner, and only the owner can modify its data. These nodes are used to track machine status, master of a cluster, taking a distributed lock. 2. **Pesistent** nodes are not deleted unless specifically requested to be deleted, used to store configuration variables. **How to elect master** - All nodes try to write to one given ephemeral node say /clusterx/master_ip, the node which is able to write will be the master as all the other writes would fail. **ZK: Setting a watch** - We can set a watch on any ZK Node, if the data on the node changes or the node gets deleted, Zookeeper notifies all the subscribers of that node. | It is used in every distributed system where we have master slave architecture. |
| Kafka - Persistent Queue | **Producer** - Systems that publish events (to a topic) are called producers. There could be multiple producers. **Consumer** - System that consume events from subscribed topic (/topics) are called consumers. **Broker** - All machines within the Kafka cluster are called brokers.**Compacted Topic** - events do not expire based on time or space bounds, newer events update older ones that possess the same key, kafka does not delete the message unless deleted by the user. **Regular Topic** - events can be configured to expire. **Partition** - Within a single topic, we can configure multiple partitions. It enables Kafka to internally shard/distributed load effectively, this also helps consumers consume faster. **Event retention period** - Kafka is designed to store events transiently and not forever. All events older than the retention period are periodically cleaned up. We can specify number of partitions for every topic, a single partition cannot be broken down between machines. **Producers can optionally specify a key along with the message being sent to the topic** this way their messages would be sent to particular partition. A **consumer group** is a collection of consumer machines, which would consume from the same topic.| Lot of applications where we need to send the success response but do few things in background asynchronously. (Eg. Notifications for messages, mails etc., Ecommerce platforms where we need to notify warehouse, invoice, email, analytics.)|

## How to approach a system design problem:
1. Figure out the MVP.  
2. Estimate scale 
    a. Storage requirements (Is sharding needed?) 
    b. Read-heavy or write-heavy system. (Write operations block read requests, as they acquire a lock on impacted rows, if system is both read and write heavy then figure out how to absorb reads or writes somewhere else.) 
    c. QPS  
3. Design Goal 
    a. Highly consistent or Highly Available. 
    b. Latency requirements 
    c. Can we afford data loss?  
4. API (the choice of sharding key may depend on the API parameters.)

## Case Studies

| S. No. | Problem | Approach |
| ----- | ---------- | --------- |
| 1 | Submit problem on Hackerrank, scaler (etc.) the input and output file is stored in File Storage which can take 2 seconds to fetch | Reading the file from Hard Disk takes 40 ms and reading a record from DB takes 50 ms. Store the file metadata in DB, with problem_id, input_filepath, input_file_updated_at, input_file_created_at and keep the filename as (problem_id)_(updated_at)_input.txt, now when the submission comes check the updated_at in the DB, if the file is present in local cache we are good, else fetch it from the filesystem. |
| 2 | How facebook computes its newsfeed (posts made by friends of the user) ? |  Posts made by user would be very less as compared to number of active users (80-20-1 rule), 1% will do posts, 80% reading, 20% interact. MAU - 1 Billion, DAU - 500 million, posts writing 1% - 5 million, lets say each person writes 4 posts daily, so we have 20 million posts every day. Each post is ~ 300 bytes, so space required for posts generated in a day 20 Mn * 300 = 6 GB. Assume only need to show posts made in last 30 days, so total space needed is 180 GB for 30 days. Hence, posts can be stored in a separate DB and retrieving becomes easier from the derived data. So recent posts, can be stored on multiple machines and separate DB for user data, so to give the recent posts we first get the friend_ids of the user, then select recent posts made by the user's friend SELECT * FROM all_posts WHERE user_id IN friend_ids LIMIT x OFFSET y. Here we are caching the posts in the HDD, not in RAM, but still this is much faster than getting the data from an actual storage system. |  
| 3 | Search Typeahead System - Max 5 suggestions required, most popular ones, strict prefix match, assume suggestions will be shown post 3 characters. Scale - 10 Billion search queries in a day, which translates to 60 billion typeahead queries in a day, if one search triggers 6 typeahead queries. Design Goal - Availability is more important than consistency, latency of getting suggestions should be super low.  Need of sharding? Assume 10% of the queries contain new search terms ~ 1 billion new search terms every day ~ 365 billion in a year, designing for 10 years 365 * 10 billion, each search term is 32 bytes and frequency is 8 bytes ~ 40 * 10 * 365 billion bytes ~ 146 TB (Sharding is needed.) | API: getSuggestion(prefix_term, limit =5), updateFrequency(search_term) If we store using Trie, so then "mic", will have a huge subtree, but we can store top five suggestions in each node, with this getSuggestion API would be quite fast, but in case of Trie we would have to take care of sharding ourselves. If we store using a HashMap, then we would need two hashmaps one for frequency, and other with top5suggestions. But with the hashmap, sharding is straight forward, but we can try to buffer the writes of updateFrequency to a separate map with the threshold, and then update the frequency in SearchTermFrequency map and PrefixTopSuggestions map if the threshold of 50 is crossed. This will take care of recency as well and optimize the number of writes. Along with this, we would also need sampling, as we are only interested in trending words rather than their exact frequency. To take recency of search queries into consideration, we can keep a Time Decay Factor to reduce the weightage of search queries performed earlier. |
| 4 | **Design Messenger** - send a message to the recipient, realtime chat, message history, most recent conversations. **Scale -** 20 Billion messages/day, single message ~ 200 bytes = 4 TB/day, so for 5 years the data will be multiple PB of storage. **Sharding** is needed, and system is both read and write heavy. System should be **Highly Consistent** and latency should be low. Write APIs should be **idempotent**, it should not create duplicate messages due to network retries. Use a messageId which is different across different messages, but same for the same message retried. | **APIs:** 1. sendMessage(sender, recipient, text, messageId) 2. getMessages(userId, conversationId, offset, limit) 3. getConversations(userId, offset, limit) 4. createUser(). **Sharding:** If we use userId as sharing key then getConversation, getMessages is fast, but sendMessage would need 2 writes. Instead, if we use conversationId as sharding key, getMessages would be fast, sendMessage would need only one write, but getConversations would be very inefficient. To overcome this we can keep a secondary DB which maintains a user to list of conversations sorted on timestamp, but with this sendMessage will have 3 writes. Most of the applications like Slack, MS Teams, Telegram etc. keep conversationId as sharding key and relax on the ordering of threads in getConversations i.e. updating the secondary DB is done as asynchronous thing in background. **Read vs write heavy** As the system needs high consistency, so we would need all the writes to be persisted immediately. We can absorb the reads by doing heavy caching, write-through cache would be useful here. The cache also needs to be sharded. Though here we have options to have the Global cache in form of Redis or we can have local cache in app servers itself. For the write heavy, consistent system we can use HBase as it provides durable storage with column-family structure perfect for message history. |
| 5 | Uber (continuation of nearest neighbors) Given a location (latitude, longtitude) match with the nearest available cabs. | If we do the sharding on city, then the scope of the problem will be reduced to few hundred thousand cabs. We can design it with the nearest neighbor approach using quad tree, storing the map in Redis, with following steps: 1. For a city, create a quadtree based on current cab locations. 2. Maintain **cab -> (last known location)** and **quadtree** backend for nearest cabs. 3. Optimize updating the quadtree and cab locations, a. Driver app only sends location if the cab location is changed in last 1 minute, and is in neighboring grid now. b. If the grid Id changes, then delete it from the old grid and update it in the new grid. c. Don't update grid dimensions immediately in the quadtree instead do it periodically. |
| 6 | Unique ID generator - The value of the ID must be incremental. We cannot use auto-increment of SQL as it would generate same IDs in distributed system, we also cannot use UUID as it is random, and not incremental. We cannot use Timestamp as in the distributed system, it can happen two request lands at the same time on different servers. | **Twitter Snowflakes Algorithm** 64 bit ID - 1 Sign bit (reserved bit set to 0), 41 Timestamp bits (epoch time, time elapsed from 4th November 2010), 5 bits for DataCenter ID (32 data centers), 5 bits for MachineID (32 machines) , 12 bits for generating sequence numbers for IDs that are generated at the same timestamp. The sequence number is reset to 0 every millisecond. |
| 7 | **Rate Limiter** - controls the rate of the traffic sent from the client to the server. It helps prevent potential DDoS attacks. **Throttling (429)** - process of controlling the usage of the APIs by customers during a given period. | 1. **Token Bucket Algorithm:** Users consume tokens to make requests, tokens are added to the bucket at a fixed rate. Though it is a bit memory intensive but still used mostly in API Gateways, handling traffic spikes while maintaining overall limits. 2. **Leaky Bucket Algorithm** - Process requests at a constant rate, regardless of incoming traffic. Maintains a hashmap of <user_id, deque>, the queue is of fixed size, requests enter the queue if there's space, else requests are rejected. It is quite memory intensive. 3. **Fixed Time Window Counter** - Counts requests in fixed time window (eg. 1 minute). The implementation is quite simple, but the major disadvantage is can allow twice the rate limit across window boundaries. 4. **Sliding Window Counter** - Considers a weighted average of current and previous window to smooth boundaries. Track request counts in the current and previous window. Calculate effective count as ```java current_count + previous_count*(overlap_percentge)``` Prevents boundary spike issues of fixed window, used where APIs need more precise rate limiting without boundary problems. 5. **Sliding Window Log** - Keep timestamp of each request and check against time window. It gives most precise rate limiting but consumes very high memory. Use case high precision rate limiting where accuracy is critical. | 